########
# Copyright (c) 2017 MSO4SC
# Author(s) javier.carnero@atos.net
#           guillaume.dolle@cemosis.fr
#           christophe.trophime@lncmi.cnrs.fr
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

tosca_definitions_version: cloudify_dsl_1_3

imports:
    # to speed things up, it is possible to download this file,
    # - https://raw.githubusercontent.com/cloudify-cosmo/cloudify-manager/18.1.11/resources/rest-service/cloudify/types/types.yaml
    - http://www.getcloudify.org/spec/cloudify/4.2.dev1/types.yaml
    # - https://raw.githubusercontent.com/MSO4SC/cloudify-hpc-plugin/master/plugin.yaml
    - https://raw.githubusercontent.com/Trophime/cloudify-hpc-plugin/master/plugin.yaml
    # # ARIA plugin:
    # - https://raw.githubusercontent.com/cloudify-cosmo/cloudify-aria-plugin/master/plugin.yaml

inputs:
    email_user:
        description: email address for reporting
        default: "christophe.trophime@lncmi.cnrs.fr"
    email_type:
        type: string
        description: define mail-type
        default: "ALL"
        # constraints:
        #     - valid_values: ["ALL", "END", "FAIL"]

    # Monitor
    monitor_entrypoint:
        description: Monitor entrypoint IP
        default: "193.144.35.146"
        type: string

    # Job prefix name
    job_prefix:
        description: Job name prefix in HPCs
        default: "mso4sc"
        type: string

    # CESGA FTII parameters
    mso4sc_hpc_primary:
        description: FTII connection credentials
        default: {}

    parallel_tasks:
        description: number of tasks/processes to run in parallel
        default: 2

    max_time:
        description: maximum allowed time for run (minutes and seconds)
        default: '00:30:00'

    hpc_partition:
        description: slurm partition to choose depending on the targeted machine
        default: 'thin-shared'
        type: string

    hpc_reservation:
        description: slurm partition to choose depending on the targeted machine
        default: ''
        type: string

    hpc_modules:
        description: modules to load depending on the targeted machine
        default:
            - gcc/6.3.0
            - openmpi/2.0.2
            - singularity/2.4.2

    # WATCH OUT : last entry should be <hpc_basedir>/feel
    hpc_basedir:
        type: string
        description: basedir directory where calculations would be performed
        default: ${LUSTRE}

    hpc_workdir_prefix:
        type: string
        description: prefix of the directory name holding the actual calculation
        default: "hifimagnet"

    hpc_feelpp:
        description: feel directory where simulations will endup
        default:
            concat: [get_input: hpc_basedir, "/feel"]

    hpc_volumes:
        description: volumes to be mounted on the targeted machine
        default:
            - /scratch
            - /mnt
            - concat: [get_input: hpc_feelpp, ":/feel"]

    # specs for running simulations
    mso4sc_dataset_input_url:
        description: url to retrieve for case file
        default: "None"


    execfile:
        type: string
        description: executable file
        default: "feelpp_hfm_thermoelectric_model_3D_V1T1_N1"

    cfgfile:
        description: configuration file
        default: "/usr/share/doc/hifimagnet/ThermoElectricModel/quarter-turn3D.cfg"
        type: string

    do_not_delete_workdir:
        description: skip deletion of workdir
        default: true
        type: boolean

    cadtype:
        description: type of cad to genererate helix or insert
        default: "helix"
        type: string

    cadcfg:
        description: name of the yaml CAD config file
        default: "HL-31_H1.yaml"
        type: string

    cadh5:
        description: name of the hdf5 mesh file
        default: "HL-31_H1_h5.json"

    # cadmeshcfg:
    #     description: name of the yaml mesh config file
    #     default: "HL-31_H1_meshdata.yaml"
    #     type: string

    cadmsh:
        description: name of the mesh file
        default: "HL-31_H1.msh"

    cadmed:
        description: name of the mesh file
        default: "HL-31_H1.med"


    # specs for retreiving singularity images
    sregistry_client:
        description: define default sregistry client
        default: "registry"

    sregistry_client_secrets:
        description: define path to file where sregistry secret are stored
        default: "$HOME/.sregistry"

    sregistry_storage:
        description: define path to container directory
        default: "${LUSTRE}/singularity_images"

    sregistry_url:
        description: URI pointing to the sregistry
        default: "sregistry.srv.cesga.es"

    sregistry_image:
        description: URI pointing to the sregistry-cli image
        default: "mso4sc/sregistry"

    # singularity image
    singularity_image_uri:
        description: URI pointing to the singularity image
        default: "hifimagnet/hifimagnet:stretch"

    singularity_image_filename:
        description: Filename of the singularity image
        default: "hifimagnet-stretch.simg"

    singularity_image_cleanup:
        # type: boolean
        description: force remove of singularity image
        default: "false"
        # default: false

    salome_hifimagnet_plugin:
        description: Path to Hifimagnet Salome plugin
        default: "/opt/SALOME-8.3.0-MPI-DB9.3/BINARIES-DB9.3/HIFIMAGNET"

    singularity_ensight_filename:
        description: Filename of the EnSight singularity image
        default: "feelpp_ensight-mso4sc.simg"

node_templates:
    primary_hpc:
        type: hpc.nodes.Compute
        properties:
            config: {get_input: mso4sc_hpc_primary}
            # external_monitor_entrypoint: {get_input: monitor_entrypoint}
            # monitor_orchestrator_available: True
            job_prefix: {get_input: job_prefix}
            base_dir: {get_input: hpc_basedir}
            workdir_prefix: {get_input: hpc_workdir_prefix}
            skip_cleanup: {get_input: do_not_delete_workdir}
            # simulate: True  # COMMENT to test against a real HPC

    create_mesh:
        type: hpc.nodes.singularity_job
        properties:
            job_options:
                mail_user: {get_input: email_user}
                mail_type: {get_input: email_type}
                modules: {get_input: hpc_modules}
                partition: {get_input: mso4sc_hpc_partition}
                reservation: {get_input: mso4sc_hpc_reservation}
                home: '${HOME}:/home/$USER'
                volumes: {get_input: hpc_volumes}
                command: {concat: ['gmsh -3 -clscale 1 -bin ', {get_input: cadmed}, ' -o,', {get_input: cadmsh}]}
                nodes: 1
                tasks: 1
                tasks_per_node: 1
                max_time: '00:15:00'
                image: {concat: [{get_input: workdir}, '/', {get_input: singularity_salome_filename}]}
            deployment:
                bootstrap: 'scripts/bootstrap_CAD.sh'
                revert: 'scripts/revert_CAD.sh'
                inputs:
                    - {get_input: workdir}
                    - {get_input: singularity_salome_filename}
                    - {get_input: datadir}
                    - {get_input: mso4sc_dataset_input_url}

        relationships:
            - type: job_contained_in_hpc
              target: primary_hpc

    partition_mesh:
        type: hpc.nodes.singularity_job
        properties:
            job_options:
                modules: {get_input: hpc_modules}
                partition: {get_input: mso4sc_hpc_partition}
                reservation: {get_input: mso4sc_hpc_reservation}
                home: '${HOME}:/home/$USER'
                volumes: {get_input: hpc_volumes}
                command: {concat: ['feelpp_mesh_partitioner --ifile ', {get_input: cadmsh}, ' --ofile ', {get_input: cadh5}, '--nochdir --part ', {get_property: [job_runmodel, job_options, tasks]}]}
                nodes: 1
                tasks: 1
                tasks_per_node: 1
                max_time: '00:15:00'
                image: {concat: [{get_input: workdir}, '/', {get_input: singularity_image_filename}]}
            deployment:
                bootstrap: 'scripts/bootstrap.sh'
                revert: 'scripts/revert.sh'
                inputs:
                    - {get_input: workdir}
                    - {get_input: singularity_image_filename}

        relationships:
            - type: job_contained_in_hpc
              target: primary_hpc
            - type: job_depends_on
              target: create_mesh

    job_runmodel:
        type: hpc.nodes.singularity_job
        properties:
            job_options:
                modules: {get_input: hpc_modules}
                partition: {get_input: mso4sc_hpc_partition}
                reservation: {get_input: mso4sc_hpc_reservation}
                home: '${HOME}:/home/$USER'
                volumes: {get_input: hpc_volumes}
                command: 'ls -lrth'
                # command: {concat: [{get_input: execfile}, ' --config-file ', {get_input: input_compute_cfg}, '.cfg']}
                # nodes: {get_input: parallel_nodes}
                tasks: {get_input: parallel_tasks}
                # tasks_per_node: {get_input: parallel_tasks_per_nodes}
                max_time: {get_input: max_time}
                image: {concat: [{get_input: workdir}, '/', {get_input: singularity_image_filename}]}
            deployment:
                bootstrap: 'scripts/bootstrap.sh'
                revert: 'scripts/revert.sh'
                inputs:
                    - {get_input: workdir}
                    - {get_input: singularity_image_filename}

        relationships:
            - type: job_contained_in_hpc
              target: primary_hpc
            - type: job_depends_on
              target: partition_mesh

outputs:
    job_runmodel:
        description: "create CAD, mesh geometry and run Simulation"
        value: {get_attribute: [job_runmodel, job_name]}
