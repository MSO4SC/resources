########
# Copyright (c) 2017 MSO4SC
# Author(s) javier.carnero@atos.net
#           guillaume.dolle@cemosis.fr
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

tosca_definitions_version: cloudify_dsl_1_3

imports:
    # to speed things up, it is possible to download this file,
    - https://raw.githubusercontent.com/cloudify-cosmo/cloudify-manager/17.10.19/resources/rest-service/cloudify/types/types.yaml
    - http://raw.githubusercontent.com/MSO4SC/cloudify-hpc-plugin/master/plugin.yaml

inputs:
    # Monitor
    monitor_entrypoint:
        description: Monitor entrypoint IP
        default: "193.144.35.146"
        type: string

    # Job prefix name
    job_prefix:
        description: Job name prefix in HPCs
        default: "mso4sc"
        type: string

    # CESGA FTII parameters
    mso4sc_hpc_primary:
        description: FTII connection credentials
        default: {}

    # SZE test infrastructure parameters
    mso4sc_hpc_secondary:
        description: SZE test infrastructure credentials
        default: {}

    parallel_nodes:
        description: number of nodes to run in parallel
        default: 1
	type: integer

    parallel_tasks_per_nodes:
        description: number of tasks per nodes
        default: 1
	type: integer

    parallel_tasks:
        description: number of tasks/processes to run in parallel
        default: 1
	type: integer
        # required: false

    max_time:
        description: maximum allowed time for run (hours, minutes and seconds)
        default: '00:30:00'
        
    input_url:
        description: url to retrieve for case file
        default: {}
	
    input_cad_type:
        description: type of cad to genererate helix or insert 
        default: "helix"
	type: string

    input_compute_exec:
        description: name of the executable to run 
        default: "feelpp_hfm_thermoelectric_model_3D_V1T1_N1"
	type: string

    input_compute_cfg:
        description: name of the simulation config file 
        default: {}
	type: string

    input_cad_cfg:
        description: name of the yaml CAD config file without extension 
        default: "HL-31_H1"
	type: string

    input_cad_mesh:
        description: name of the mesh file without extension 
        default: "HL-31_H1"

node_templates:
    primary_hpc:
        type: hpc.nodes.Compute
        properties:
            config: { get_input: mso4sc_hpc_primary }
            monitor_entrypoint: { get_input: monitor_entrypoint }
            monitor_orchestrator_available: True
            job_prefix: { get_input: job_prefix }
#            simulate: True  # COMMENT to test against a real HPC

    create_geometry:
        type: hpc.nodes.singularity_job
        properties:
            job_options:
                modules:
                    - gcc/6.1.0
                    - openmpi/1.10.2
                    - singularity/2.4.2
                partition: 'thin-shared'
                home: '${HOME}:/home/$USER'
                volumes:
                    - '/scratch'
                    - '${LUSTRE}/feel:/feel'
                    - '${STORE}/Distene:/opt/DISTENE/DLim`
                command: { concat: [ 'salome -t ~/SALOME-8.3.0-DB9.3/INSTALL/HIFIMAGNET/bin/salome/HIFIMAGNET_Cmd.py ',
                                      'args:--',
                                      {get_input: input_cad_type},'=',
				      {get_input: input_cad_cfg},'.yaml',
				      ',--mesh' ]}
		# command: 'cd /data && salome
		#     -t ~/SALOME-8.3.0-DB9.3/INSTALL/HIFIMAGNET/bin/salome/HIFIMAGNET_Cmd.py
		#     args:--helix=HL-31_H1.yaml,--mesh'
                nodes: 1
                tasks: 1
                tasks_per_node: 1
                max_time: '00:15:00'
                image: '${STORE}/Singularity/feelpp_salome-mso4sc.simg'
            deployment:
                bootstrap: 'scripts/bootstrap.sh'
                revert: 'scripts/revert.sh'
                inputs:
                    - '${STORE}/Singularity/' # boostrap, revert scripts arg $1
                    - 'feelpp_salome-mso4sc.simg' # bootstrap, revert scripts arg $2
		    - '${STORE}/HIFIMAGNET/demos/HL-dble'

        relationships:
            - type: job_contained_in_hpc
              target: primary_hpc

    create_mesh:
        type: hpc.nodes.singularity_job
        properties:
            job_options:
                modules:
                    - gcc/6.1.0
                    - openmpi/1.10.2
                    - singularity/2.4.2
                partition: 'thin-shared'
                home: '${HOME}:/home/$USER'
                volumes:
                    - '/scratch'
                    - '${LUSTRE}/feel:/feel'
                command: { concat: [ 'gmsh -3 -clscale 1 -bin ',{get_input: input_yaml_cfg},'.med -o,',{get_input: input_yaml_cfg},'.msh' ]}
		# command: 'cd /data && gmsh -3 -clscale 1 -bin
		#     -o HL-31_H1{get_input: input_yaml_cfg}.msh'
                nodes: 1
                tasks: 1
                tasks_per_node: 1
                max_time: '00:15:00'
                image: '${STORE}/Singularity/feelpp_salome-mso4sc.simg'
            deployment:
                bootstrap: 'scripts/bootstrap.sh'
                revert: 'scripts/revert.sh'
                inputs:
                    - '${STORE}/Singularity/' # boostrap, revert scripts arg $1
                    - 'feelpp_salome-mso4sc.simg' # bootstrap, revert scripts arg $2
		    - '${STORE}/HIFIMAGNET/demos/HL-dble'

        relationships:
            - type: job_contained_in_hpc
              target: primary_hpc
            - type: job_depends_on
              target: create_geometry

    partition_mesh:
        type: hpc.nodes.singularity_job
        properties:
            job_options:
                modules:
                    - gcc/6.1.0
                    - openmpi/1.10.2
                    - singularity/2.4.2
                partition: 'thin-shared'
                home: '${HOME}:/home/$USER'
                volumes:
                    - '/scratch'
                    - '${LUSTRE}/feel:/feel'
                    - '${STORE}/HIFIMAGNET/demos/HL-dble:/data`
                command: { concat:[
                    'feelpp_mesh_partitioner
                     --ifile ', { get_input: input_cad_cfg}, '.msh
                     --nochdir
                     --part ', { get_property: [job_thermoelectricmodel, job_options ,tasks] }] }
                nodes: 1
                tasks: 1
                tasks_per_node: 1
                max_time: '00:15:00'
                image: '${STORE}/Singularity/feelpp_hifimagnet-mso4sc.simg'
            deployment:
                bootstrap: 'scripts/bootstrap.sh'
                revert: 'scritps/revert.sh'
                inputs:
                    - '${STORE}/Singularity' # boostrap, revert scripts arg $1
                    - 'feelpp_hifimagnet-mso4sc.simg' # bootstrap, revert scripts arg $2

        relationships:
            - type: job_contained_in_hpc
              target: primary_hpc
            - type: job_depends_on
              target: create_mesh

    job_thermoelectricmodel:
        type: hpc.nodes.singularity_job
        properties:
            job_options:
                modules:
                    - gcc/6.1.0
                    - openmpi/1.10.2
                    - singularity/2.4.2
                partition: 'thin-shared'
		home: '${HOME}:/home/$USER'
                volumes:
                    - '/scratch'
                    - '${LUSTRE}/feel:/feel'
                command: { concat: [{get_input: input_compute_exec},' --config-file ',{get_input: input_compute_cfg},'.cfg']}
                nodes: { get_input: parallel_nodes }
                tasks: { get_input: parallel_tasks }
                tasks_per_node: { get_input: parallel_tasks_per_nodes }
                max_time: { get_input: max_time }
                image: '${STORE}/Singularity/feelpp_hifimagnet-mso4sc.simg'
            deployment:
                bootstrap: 'scripts/bootstrap.sh'
                revert: 'scripts/revert.sh'
                inputs:
                    - '${STORE}/Singularity' # boostrap, revert scripts arg $1
                    - 'feelpp_hifimagnet-mso4sc.simg' # bootstrap, revert scripts arg $2
		    - '${STORE}/HIFIMAGNET/demos/HL-dble'

        relationships:
            - type: job_contained_in_hpc
              target: primary_hpc
            - type: job_depends_on
              target: partition_mesh

outputs:
    job_thermoelectricmodel:
        description: feelpp_hfm_thermoelectric_model_3D_V1T1_N1 results
        value: { get_attribute: [job_thermoelectricmodel, job_name] }
