########
# Copyright (c) 2017 MSO4SC
# Author(s) javier.carnero@atos.net
#           guillaume.dolle@cemosis.fr
#           christophe.trophime@lncmi.cnrs.fr
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

tosca_definitions_version: cloudify_dsl_1_3

imports:
    # to speed things up, it is possible to download this file,
    # - https://raw.githubusercontent.com/cloudify-cosmo/cloudify-manager/18.1.11/resources/rest-service/cloudify/types/types.yaml
    - http://raw.githubusercontent.com/mso4sc/cloudify-hpc-plugin/master/resources/types/cfy_types.yaml
    # HPC pluging
    - http://raw.githubusercontent.com/MSO4SC/cloudify-hpc-plugin/master/plugin.yaml
    # - http://raw.githubusercontent.com/MSO4SC/cloudify-hpc-plugin/canary/plugin.yaml
    # - https://raw.githubusercontent.com/Trophime/cloudify-hpc-plugin/add_checks/plugin.yaml

inputs:
    email_user:
        description: email address for reporting
        default: "first.lastname@domain"

    # CESGA FTII parameters
    mso4sc_hpc_primary:
        description: FTII connection credentials
        default: {}

    resource_hpc_parallel_nodes:
        description: number of nodes to run in parallel(optional)
        default: ''

    resource_hpc_parallel_tasks_per_node:
        description: number of tasks per node (optional)
        default: ''

    resource_hpc_parallel_tasks:
        type: integer
        description: number of tasks to run in parallel (mandatory)
        default: 1

    resource_hpc_max_time:
        description: maximum allowed time for run (minutes and seconds)
        default: '00:30:00'

    resource_hpc_partition:
        type: string
        description: slurm partition to choose depending on the targeted machine
        default: 'thinnodes'

    resource_hpc_reservation:
        type: string
        description: slurm partition to choose depending on the targeted machine
        default: ''

    resource_hpc_modules:
        description: modules to load depending on the targeted machine
        default:
            - gcc/6.3.0
            - openmpi/2.0.2
            - singularity/2.4.2

    resource_hpc_basedir:
        type: string
        description: basedir directory where calculations would be performed
        default: ${LUSTRE}

    hpc_feelpp:
        type: string
        description: feel directory where simulations will endup
        default: ${LUSTRE}/feel

    resource_hpc_volumes:
        description: volumes to be mounted on the targeted machine
        default:
            - /scratch
            - /mnt
            - ${LUSTRE}/feel:/feel

    # specs for running simulations
    mso4sc_dataset_model:
        description: Model dataset
        default: "None"

    mso4sc_datacatalogue_entrypoint:
        description: entrypoint of the data catalogue
        default: "http://193.144.35.207"

    mso4sc_datacatalogue_key:
        description: API Key to publish the outputs
        default: ""

    mso4sc_outdataset_outputs_at:
        description: ID of the CKAN output dataset
        default: ""

    cfgfile:
        type: string
        description: configuration file
        default: "quarter-turn3D.cfg"

    workdir_keep:
        type: boolean
        description: skip deletion of workdir
        default: true

    # # specs for retreiving singularity images
    sregistry_storage:
        description: define path to container directory
        default: "${LUSTRE}/singularity_images"

dsl_definitions:
    - &monitor_entrypoint #Monitor entrypoint IP
      "193.144.35.146"
    - &job_prefix #Job name prefix in HPCs
      "hifimagnet"
    - &email_type # define mail-type ADD, END, FAIL
      "ALL"
    - &hpc_workdir_prefix
      "thermoelec"
    - &sregistry_client
      "registry"
    - &sregistry_client_secrets
      "$HOME/.sregistry"
    - &sregistry_url
      "sregistry.srv.cesga.es"
    - &sregistry_image
      "mso4sc/sregistry:latest"
    - &singularity_image_uri
      "hifimagnet/hifimagnet:v0.103"
    - &singularity_image_filename
      "hifimagnet-hifimagnet-v0.103.simg"
    - &singularity_image_cleanup
      "false"
    - &execfile
      "feelpp_hfm_thermoelectric_model_3D_V1T1_N1"

node_templates:
    primary_hpc:
        type: hpc.nodes.Compute
        properties:
            config: {get_input: mso4sc_hpc_primary}
            # external_monitor_entrypoint: *monitor_entrypoint
            # monitor_orchestrator_available: True
            job_prefix: *job_prefix
            base_dir: {get_input: resource_hpc_basedir}
            workdir_prefix: *hpc_workdir_prefix
            skip_cleanup: {get_input: workdir_keep}
            # simulate: True  # COMMENT to test against a real HPC

    job_partition:
        type: hpc.nodes.singularity_job
        properties:
            job_options:
                mail_user: {get_input: email_user}
                mail_type: *email_type
                modules: {get_input: resource_hpc_modules}
                image: {concat: [{get_input: sregistry_storage}, '/', *singularity_image_filename]}
                home: '${PWD}:/home/${USER}'
                volumes: {get_input: resource_hpc_volumes}
                #command: {concat: ['feelpp_mesh_partitioner --gmsh.scale=0.001 --ifile ', {get_input: cadmsh}, '.msh --ofile ', {get_input: cadmsh} , '_p.json --part ', {get_input: resource_hpc_parallel_tasks}, '  > partition.log 2>&1']}
                # partition.sh is shipped with dataset
                command: {concat: ['/home/${USER}/partition.sh -i ', {get_input: cfgfile}, ' -n ', {get_input: resource_hpc_parallel_tasks}, ' > partition.log 2>&1']}
                partition: {get_input: resource_hpc_partition}
                reservation: {get_input: resource_hpc_reservation}
                nodes: 1
                tasks: 1
                tasks_per_node: 1
                max_time: {get_input: resource_hpc_max_time}
            skip_cleanup: {get_input: workdir_keep}
            deployment:
                bootstrap: 'scripts/bootstrap.sh'
                revert: 'scripts/revert.sh'
                inputs:
                    - {get_input: sregistry_storage}
                    - *singularity_image_filename
                    - *singularity_image_uri
                    - *singularity_image_cleanup
                    - *sregistry_client
                    - *sregistry_client_secrets
                    - *sregistry_url
                    - *sregistry_image
                    - {get_input: hpc_feelpp}
                    - {get_input: mso4sc_datacatalogue_key}
                    - {get_input: mso4sc_dataset_model}
                    - {get_input: cfgfile}

        relationships:
            - type: job_contained_in_hpc
              target: primary_hpc
            # - type: job_depends_on
            #   target: job_convert

    job_sim:
        type: hpc.nodes.singularity_job
        properties:
            job_options:
                mail_user: {get_input: email_user}
                mail_type: *email_type
                modules: {get_input: resource_hpc_modules}
                image: {concat: [{get_input: sregistry_storage}, '/', *singularity_image_filename]}
                home: '${HOME}:/home/${USER}'
                volumes: {get_input: resource_hpc_volumes}
                command: {concat: [*execfile, ' --config-file ', {get_input: cfgfile}, ' > sim.log 2>&1']}
                partition: {get_input: resource_hpc_partition}
                reservation: {get_input: resource_hpc_reservation}
                nodes: {get_input: resource_hpc_parallel_nodes}
                tasks: {get_input: resource_hpc_parallel_tasks}
                tasks_per_node: {get_input: resource_hpc_parallel_tasks_per_node}
                max_time: {get_input: resource_hpc_max_time}
            skip_cleanup: {get_input: workdir_keep}
            deployment:
                bootstrap: 'scripts/bootstrap.sh'
                revert: 'scripts/revert.sh'
                inputs:
                    - {get_input: sregistry_storage}
                    - *singularity_image_filename
                    - *singularity_image_uri
                    - *singularity_image_cleanup
                    - *sregistry_client
                    - *sregistry_client_secrets
                    - *sregistry_url
                    - *sregistry_image
                    - {get_input: hpc_feelpp}
                    - {get_input: mso4sc_datacatalogue_key}
                    - {concat: ['"', {get_input: mso4sc_dataset_model}, '"']}
                    - {get_input: cfgfile}

            # /feel/hifimagnet/ThermoElectricModel/HL-31_H1_p.json/thermoelectric-nonlinear_V1T1_N1/np_32
            publish:
                - type: "CKAN"
                  entrypoint: {get_input: mso4sc_datacatalogue_entrypoint}
                  api_key: {get_input: mso4sc_datacatalogue_key}
                  dataset: {get_input: mso4sc_outdataset_outputs_at}
                  file_path:
                      - {concat: [{get_input: hpc_feelpp}, '/hifimagnet/ThermoElectricModel/']}
                  name: 'ThermoElectricModel'
                  description: {concat: ['ThermoElectricModel: ', {get_input: cfgfile}, ', ', {get_input: resource_hpc_parallel_tasks}, ' parts']}

        relationships:
            - type: job_contained_in_hpc
              target: primary_hpc
            # - type: job_depends_on
            #   target: job_convert
            - type: job_depends_on
              target: job_partition

outputs:
    job_partition:
        description: feelpp_mesh_partitioner results
        value: {get_attribute: [job_partition, job_name]}
    job_sim:
        description: hifimagnet thermoelec results
        value: {get_attribute: [job_sim, job_name]}
