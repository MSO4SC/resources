= Hifimagnet basic workflow TOSCA

The `upload` directory is uploaded on the orchestrator

WARNING: Don't put your local in the upload directory to  avoid uploading your credentials (eg. `user/pass`)

You can copy the singularity image in your `${LUSTRE}/singularity_images` folder
if it's not available the ${SINGULARITY_REPO} directory. (module load singularity on ft2)

First clone the `resources`:
[source]
----
mkdir -p MSO4SC && cd $_
git clone https://github.com/MSO4SC/resources.git
git pull MSO4SC/resources
----

Set your local blueprint file (`user/password`) at the root of the cloned repository
named "local-blueprint-inputs.yaml" (It is ignored by git, then you won't risk to push
it on the remote repository)

See http://book.mso4sc.cemosis.fr/#mso4sc_tosca_modelling_and_execution

You also need to configure a few key:

.List of keys to be defined for HPC settings
[options="header,footer,autowidth"]
|===
| Key                        | Description              | Default          | Notes
| monitor_entrypoint         |  Monitor entrypoint IP   | "193.144.35.146" |
| job_prefix                 | Job name prefix in HPCs | "mso4sc"         |
| mso4sc_hpc_primary         | HPC settings | | 
| hpc_modules | modules to load depending on the targeted machine | [gcc/6.3.0, openmpi/2.0.2, singularity/2.4.2] | shall depends on HPC and sing image
| hpc_volumes | volumes to be mounted on the targeted machine |  [/scratch,/mnt,${LUSTRE}/feel:/feel] | depends on HPC and sing. image
| hpc_partition | select partition queue | 'thin-shared' | shall depends on batch system and HPC (TODO)
| hpc_reservation | select reservation queue | `` | is optional
| parallel_tasks |  number of tasks/processes to run in parallel | 2 |  shall depends on batch system and selected partition
| max_time | maximum allowed time for run (minutes and seconds) | '00:30:00' |  shall depends on batch system and selected partition
|===

.List of keys for defining HPC `mso4sc_hpc_primary`:
[options="header,footer,autowidth"]
|===
| Key                        | Description                   | Default          | Notes
| credentials                |                               |                  |
| -- host                    | full qualified hostname or IP | `ft2.cesga.es`   |
| -- user                    | username                      |                  |
| -- password                | password                      |                  |
| country_tz                 | country timezone              | `Europe/Madrid`  |
| workload_manager           | batch system                  | `SLURM`          | `TORQUE` is also implemented
| base_dir                   | `$LUSTRE`                     |                  |
|===

.List of keys specs for running simulations
[options="header,footer,autowidth"]
|===
| Key                        | Description              | Default          | Notes
| mso4sc_dataset_input_url | url to retrieve for cfg file and input data |  `None`

| execfile | executable file | `feelpp_hfm_thermoelectric_model_3D_V1T1_N1` |
| cfgfile |  configuration file | `/usr/share/doc/hifimagnet/ThermoElectricModel/quarter-turn3D.cfg` |
|===

.List of keys specs for retreiving singularity images
[options="header,footer,autowidth"]
|===
| Key                        | Description              | Default          | Notes
| sregistry_client           | define default sregistry client                |  `registry` | 
| sregistry_client_secrets   | path to file where sregistry secret are stored | `$HOME/.sregistry` |
| sregistry_storage          | path to container directory                    | `${LUSTRE}/singularity_images` |
| sregistry_url              | URI pointing to the sregistry                  | `sregistry.srv.cesga.es` |
| sregistry_image            | URI pointing to the sregistry-cli image        | `mso4sc/sregistry` |
|===

.List of keys specs for singularity image
[options="header,footer,autowidth"]
|===
| Key                        | Description              | Default          | Notes
| singularity_image_uri      |  URI pointing to the singularity image   | `hifimagnet/hifimagnet:stretch` |
| singularity_image_filename |  Filename of the singularity image       | `hifimagnet-stretch.simg` | 
| singularity_image_cleanup  | force remove of singularity image        | `false` |
|===

Connect to the cloudify client via docker:
[source]
----
docker pull mso4sc/msoorchestrator-cli
docker run -it --rm \
    -v $HOME/MSO4SC/resources:/resources \
     mso4sc/msoorchestrator-cli
----

Within the docker:
[source]
----
cfy profiles use  ORCHESTRATOR_IP -t  default_tenant -u USER -p PASSWD
----

Then you can simply deploy your application using:
[source]
----
./deploy up
----

To undeploy on the orchestrator, just:
[source]
----
./deploy down
----

NOTE: If the application fails during one step of the deploy up process, you'll have to
force cancel execution by hands. `cfy executions list`, `cfy executions cancel -f <id>`
in the orchestrator client (*docker).

See https://github.com/MSO4SC/MSOOrchestrator-CLI/blob/master/README.adoc#remote-mode for details


