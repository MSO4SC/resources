= Torsionbar TOSCA

Generate a set of very simple blueprint for different Feel++ toolboxes.

== Command-line usage

The "upload" is uploaded on the orchestrator

WARNING: Don't put your local in the upload directory to not upload your user/pass

You can copy the singularity image in your ${LUSTRE} folder if it's not available
the ${SINGULARITY_REPO} directory. (module load singularity on ft2)

Connect to the cloudify client via docker.
See https://github.com/MSO4SC/MSOOrchestrator-CLI/blob/master/README.adoc

Set your local blueprint file (user/password) at the root of the cloned repository
named "local-blueprint-inputs.yaml" (It is ignored by git, then you won't risk to push
it on the remote repository)
See http://book.mso4sc.cemosis.fr/#mso4sc_tosca_modelling_and_execution

[source]
----
./deploy up
----

to deploy on the orchestrator. <config> is one of the base upload/config/<config>.yaml
file.

[source]
----
./deploy down
----

NOTE: If the application fails during one step of the deploy up process, you'll have to
force cancel execution by hands. `cfy executions list`, `cfy executions cancel -f <id>`
in the orchestrator client (*docker).

== Portal

To use this blueprint with the portal, you need to archive the application upload
on the data management tool (CKAN) or any others tool available (zenodo, girder for example).
When you create a new catalog entry in the marketplace, you need to provide the
path to this archive via the _BLUEPRINT_PATH_ variable set as a *characteristic*

[source]
----
./deploy pkg
----

The archive is uploaded on https://girder.math.unistra.fr/#folder/5a6261a1b0e9570150cb23ef
and used in the _BLUEPRINT_PATH_ characteristic in the market place for the
eye2brain product.

Finally, to generate each toolbox, just type
[source]
----
./deploy build
----
All archive will be available in the build directory.

=== HPC select.

Currently, bluescripts are not adapted to several HPC and might changes in the future.
You can add in the portal a new HPC, for example Finit Terrae II, or for unistra users
Atlas (be careful to use the specific user, ask to admins associated to the project).

Once done, you can set particular machine inputs depending on the machine.

====  FT2

[source]
----
job_hpc_modules: ['gcc/6.1.0','openmpi/1.10.2','singularity/2.4.2']
job_hpc_partition: "thin-shared"
job_hpc_workdir: "${HOME}/mso4sc_orchestrator"
job_hpc_simg_mount: ['/data', '/mnt', '${HOME}/mso4sc_orchestrator/feel:/feel']
----

==== atlas

[source]
----
job_hpc_modules: ['gcc/6.4.0','openmpi/1.10.7_gcc640']
job_hpc_partition: "public"
job_hpc_workdir: "${HOME}/mso4sc_orchestrator"
job_hpc_simg_mount: ['/data', '/mnt', '${HOME}/mso4sc_orchestrator/feel:/feel']
----

== Debug

=== Blueprint

You can uncomment the avoid_cleanup to check the temporary script generated by the
orchestrator.
The SLURM batch script won't be cleaned up as well as the bootrap scripts.
You can connect to the targetted cluster to check these scripts in the ${HOME}
directory.

=== Bootstrap and revert

The bootstrap script debug info might be complex to be retrieved in current version.
For bootstrap version using girder, the script will return error code corresponding
to the link:https://curl.haxx.se/libcurl/c/libcurl-errors.html[curl error code].

Exit code 1 can mean several things. Check first the script call printed in the
orchestrator console. Often, it means a local variable was not set and argument is
then missing or wrong in the bootstrap script.
Else execute the call command directly on the cluster to debug the script.

All bootstrap/revert logs are generated in the `${HOME}` directory prefixed by
`bootstrap_<jobname>_<jobid>` and `revert_<jobname>_<jobid>`.



